<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta charset="utf-8">
<meta name="generator" content="ReSpec 26.13.4">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<style>
.issue-label{text-transform:initial}
.warning>p:first-child{margin-top:0}
.warning{padding:.5em;border-left-width:.5em;border-left-style:solid}
span.warning{padding:.1em .5em .15em}
.issue.closed span.issue-number{text-decoration:line-through}
.warning{border-color:#f11;border-width:.2em;border-style:solid;background:#fbe9e9}
.warning-title:before{content:"⚠";font-size:1.3em;float:left;padding-right:.3em;margin-top:-.3em}
li.task-list-item{list-style:none}
input.task-list-item-checkbox{margin:0 .35em .25em -1.6em;vertical-align:middle}
.issue a.respec-gh-label{padding:5px;margin:0 2px 0 2px;font-size:10px;text-transform:none;text-decoration:none;font-weight:700;border-radius:4px;position:relative;bottom:2px;border:none;display:inline-block}
</style>
<style>
dfn{cursor:pointer}
.dfn-panel{position:absolute;z-index:35;min-width:300px;max-width:500px;padding:.5em .75em;margin-top:.6em;font:small Helvetica Neue,sans-serif,Droid Sans Fallback;background:#fff;color:#000;box-shadow:0 1em 3em -.4em rgba(0,0,0,.3),0 0 1px 1px rgba(0,0,0,.05);border-radius:2px}
.dfn-panel:not(.docked)>.caret{position:absolute;top:-9px}
.dfn-panel:not(.docked)>.caret::after,.dfn-panel:not(.docked)>.caret::before{content:"";position:absolute;border:10px solid transparent;border-top:0;border-bottom:10px solid #fff;top:0}
.dfn-panel:not(.docked)>.caret::before{border-bottom:9px solid #a2a9b1}
.dfn-panel *{margin:0}
.dfn-panel b{display:block;color:#000;margin-top:.25em}
.dfn-panel ul a[href]{color:#333}
.dfn-panel>div{display:flex}
.dfn-panel a.self-link{font-weight:700;margin-right:auto}
.dfn-panel .marker{padding:.1em;margin-left:.5em;border-radius:.2em;text-align:center;white-space:nowrap;font-size:90%;color:#040b1c}
.dfn-panel .marker.dfn-exported{background:#d1edfd;box-shadow:0 0 0 .125em #1ca5f940}
.dfn-panel .marker.idl-block{background:#8ccbf2;box-shadow:0 0 0 .125em #0670b161}
.dfn-panel a:not(:hover){text-decoration:none!important;border-bottom:none!important}
.dfn-panel a[href]:hover{border-bottom-width:1px}
.dfn-panel ul{padding:0}
.dfn-panel li{margin-left:1em}
.dfn-panel.docked{position:fixed;left:.5em;top:unset;bottom:2em;margin:0 auto;max-width:calc(100vw - .75em * 2 - .5em - .2em * 2);max-height:30vh;overflow:auto}
</style>
	  
	  
<title>Synchronization Accessibility User Requirements</title>
	  		
		
		
	
<style id="respec-mainstyle">
@keyframes pop{
0%{transform:scale(1,1)}
25%{transform:scale(1.25,1.25);opacity:.75}
100%{transform:scale(1,1)}
}
.hljs{background:0 0!important}
a abbr,h1 abbr,h2 abbr,h3 abbr,h4 abbr,h5 abbr,h6 abbr{border:none}
dfn{font-weight:700}
a.internalDFN{color:inherit;border-bottom:1px solid #99c;text-decoration:none}
a.externalDFN{color:inherit;border-bottom:1px dotted #ccc;text-decoration:none}
a.bibref{text-decoration:none}
.respec-offending-element:target{animation:pop .25s ease-in-out 0s 1}
.respec-offending-element,a[href].respec-offending-element{text-decoration:red wavy underline}
@supports not (text-decoration:red wavy underline){
.respec-offending-element:not(pre){display:inline-block}
.respec-offending-element{background:url(data:image/gif;base64,R0lGODdhBAADAPEAANv///8AAP///wAAACwAAAAABAADAEACBZQjmIAFADs=) bottom repeat-x}
}
#references :target{background:#eaf3ff;animation:pop .4s ease-in-out 0s 1}
cite .bibref{font-style:normal}
code{color:#c63501}
th code{color:inherit}
a[href].orcid{padding-left:4px;padding-right:4px}
a[href].orcid>svg{margin-bottom:-2px}
.toc a,.tof a{text-decoration:none}
a .figno,a .secno{color:#000}
ol.tof,ul.tof{list-style:none outside none}
.caption{margin-top:.5em;font-style:italic}
table.simple{border-spacing:0;border-collapse:collapse;border-bottom:3px solid #005a9c}
.simple th{background:#005a9c;color:#fff;padding:3px 5px;text-align:left}
.simple th a{color:#fff;padding:3px 5px;text-align:left}
.simple th[scope=row]{background:inherit;color:inherit;border-top:1px solid #ddd}
.simple td{padding:3px 10px;border-top:1px solid #ddd}
.simple tr:nth-child(even){background:#f0f6ff}
.section dd>p:first-child{margin-top:0}
.section dd>p:last-child{margin-bottom:0}
.section dd{margin-bottom:1em}
.section dl.attrs dd,.section dl.eldef dd{margin-bottom:0}
#issue-summary>ul{column-count:2}
#issue-summary li{list-style:none;display:inline-block}
details.respec-tests-details{margin-left:1em;display:inline-block;vertical-align:top}
details.respec-tests-details>*{padding-right:2em}
details.respec-tests-details[open]{z-index:999999;position:absolute;border:thin solid #cad3e2;border-radius:.3em;background-color:#fff;padding-bottom:.5em}
details.respec-tests-details[open]>summary{border-bottom:thin solid #cad3e2;padding-left:1em;margin-bottom:1em;line-height:2em}
details.respec-tests-details>ul{width:100%;margin-top:-.3em}
details.respec-tests-details>li{padding-left:1em}
a[href].self-link:hover{opacity:1;text-decoration:none;background-color:transparent}
h2,h3,h4,h5,h6{position:relative}
aside.example .marker>a.self-link{color:inherit}
h2>a.self-link,h3>a.self-link,h4>a.self-link,h5>a.self-link,h6>a.self-link{border:none;color:inherit;font-size:83%;height:2em;left:-1.6em;opacity:.5;position:absolute;text-align:center;text-decoration:none;top:0;transition:opacity .2s;width:2em}
h2>a.self-link::before,h3>a.self-link::before,h4>a.self-link::before,h5>a.self-link::before,h6>a.self-link::before{content:"§";display:block}
@media (max-width:767px){
dd{margin-left:0}
h2>a.self-link,h3>a.self-link,h4>a.self-link,h5>a.self-link,h6>a.self-link{left:auto;top:auto}
}
@media print{
.removeOnSave{display:none}
}
</style>
<meta name="description" content="This document summarizes relevant research, then outlines accessibility-related user needs and associated requirements for the synchronization of audio and visual media. The scope of the discussion includes synchronization of accessibility-related components of multimedia, such as captions, sign language interpretation, and descriptions. The requirements identified herein are applicable to multimedia content in general, as well as real-time communication applications and media occurring in immersive environments.">
<link rel="canonical" href="https://www.w3.org/TR/saur/">
<style>
var{position:relative;cursor:pointer}
var[data-type]::after,var[data-type]::before{position:absolute;left:50%;top:-6px;opacity:0;transition:opacity .4s;pointer-events:none}
var[data-type]::before{content:"";transform:translateX(-50%);border-width:4px 6px 0 6px;border-style:solid;border-color:transparent;border-top-color:#000}
var[data-type]::after{content:attr(data-type);transform:translateX(-50%) translateY(-100%);background:#000;text-align:center;font-family:"Dank Mono","Fira Code",monospace;font-style:normal;padding:6px;border-radius:3px;color:#daca88;text-indent:0;font-weight:400}
var[data-type]:hover::after,var[data-type]:hover::before{opacity:1}
</style>
<script id="initialUserConfig" type="application/json">{
  "trace": true,
  "useExperimentalStyles": true,
  "doRDFa": "1.1",
  "includePermalinks": true,
  "permalinkEdge": true,
  "permalinkHide": false,
  "noRecTrack": true,
  "tocIntroductory": true,
  "specStatus": "ED",
  "diffTool": "http://www.aptest.com/standards/htmldiff/htmldiff.pl",
  "shortName": "saur",
  "copyrightStart": "2021",
  "license": "w3c-software-doc",
  "editors": [
    {
      "name": "Steve Noble",
      "company": "Pearson",
      "companyURI": "https://www.pearson.com/",
      "w3cid": 81519
    },
    {
      "name": "Jason White",
      "url": "http://www.ets.org/",
      "company": "Educational Testing Service",
      "companyURI": "http://www.ets.org/",
      "w3cid": 74028
    },
    {
      "name": "Scott Hollier",
      "url": "http://www.hollier.info/",
      "w3cid": 43274
    },
    {
      "name": "Janina Sajka",
      "url": "http://rednote.net/",
      "w3cid": 33688
    },
    {
      "name": "Joshue O'Connor",
      "url": "https://www.w3.org",
      "company": "W3C",
      "companyURI": "https://www.w3.org",
      "w3cid": 41218
    }
  ],
  "group": "apa",
  "github": "w3c/apa",
  "maxTocLevel": 4,
  "publishDate": "2021-09-28",
  "publishISODate": "2021-09-28T00:00:00.000Z",
  "generatedSubtitle": "Editor's Draft 28 September 2021"
}</script>
<link rel="stylesheet" href="https://www.w3.org/StyleSheets/TR/2016/W3C-ED"></head>
	<body class="h-entry informative"><div class="head">
    <a class="logo" href="https://www.w3.org/"><img crossorigin="" alt="W3C" height="48" src="https://www.w3.org/StyleSheets/TR/2016/logos/W3C" width="72">
  </a> <h1 id="title" class="title">Synchronization Accessibility User Requirements</h1>
    
    <h2>
      W3C Editor's Draft
      <time class="dt-published" datetime="2021-09-28">28 September 2021</time>
    </h2>
    <dl>
      <dt>This version:</dt><dd>
              <a class="u-url" href="https://w3c.github.io/apa/">https://w3c.github.io/apa/</a>
            </dd><dt>Latest published version:</dt><dd>
              <a href="https://www.w3.org/TR/saur/">https://www.w3.org/TR/saur/</a>
            </dd>
      <dt>Latest editor's draft:</dt><dd><a href="https://w3c.github.io/apa/">https://w3c.github.io/apa/</a></dd>
      
      
      
      
      
      <dt>Editors:</dt>
      <dd class="editor p-author h-card vcard" data-editor-id="81519">
    <span class="p-name fn">Steve Noble</span> (<span class="p-org org h-org">Pearson</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="74028">
    <a class="u-url url p-name fn" href="http://www.ets.org/">Jason White</a> (<span class="p-org org h-org">Educational Testing Service</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="43274">
    <a class="u-url url p-name fn" href="http://www.hollier.info/">Scott Hollier</a>
  </dd><dd class="editor p-author h-card vcard" data-editor-id="33688">
    <a class="u-url url p-name fn" href="http://rednote.net/">Janina Sajka</a>
  </dd><dd class="editor p-author h-card vcard" data-editor-id="41218">
    <a class="u-url url p-name fn" href="https://www.w3.org">Joshue O'Connor</a> (<span class="p-org org h-org">W3C</span>)
  </dd>
      
      
      <dt>Participate:</dt><dd>
    <a href="https://github.com/w3c/apa/">GitHub w3c/apa</a>
  </dd><dd>
    <a href="https://github.com/w3c/apa/issues/">File an issue</a>
  </dd><dd>
    <a href="https://github.com/w3c/apa/commits/">Commit history</a>
  </dd><dd>
    <a href="https://github.com/w3c/apa/pulls/">Pull requests</a>
  </dd>
    </dl>
    
    
    
    <p class="copyright">
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a>
    ©
    2021
    
    <a href="https://www.w3.org/"><abbr title="World Wide Web Consortium">W3C</abbr></a><sup>®</sup> (<a href="https://www.csail.mit.edu/"><abbr title="Massachusetts Institute of Technology">MIT</abbr></a>,
    <a href="https://www.ercim.eu/"><abbr title="European Research Consortium for Informatics and Mathematics">ERCIM</abbr></a>, <a href="https://www.keio.ac.jp/">Keio</a>,
    <a href="https://ev.buaa.edu.cn/">Beihang</a>). 
    W3C <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>,
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and <a rel="license" href="https://www.w3.org/Consortium/Legal/2015/copyright-software-and-document">permissive document license</a> rules
    apply.
  </p>
    <hr title="Separator for header">
  </div>
	  <section id="abstract" class="introductory">
	  	<h2>Abstract</h2>
	    <p>This document summarizes relevant research, then outlines accessibility-related user needs and associated requirements for the synchronization of audio and visual media. The scope of the discussion includes synchronization of accessibility-related components of multimedia, such as captions, sign language interpretation, and descriptions. The requirements identified herein are applicable to multimedia content in general, as well as real-time communication applications and media occurring in immersive environments.</p>
	    <p>The purpose of this document is to identify and to characterize synchronization-related needs. It does not constitute normative guidance. It may, nevertheless, influence the further development of <abbr title="World Wide Web Consortium">W3C</abbr> specifications, including accessibility guidelines and media-related technologies. It may also be applied to the development of multimedia content and applications to enhance accessibility.</p>
	  </section>
	  <section id="sotd" class="introductory"><h2>Status of This Document</h2><p><em>This section describes the status of this
      document at the time of its publication. Other documents may supersede
      this document. A list of current <abbr title="World Wide Web Consortium">W3C</abbr> publications and the latest revision
      of this technical report can be found in the
      <a href="https://www.w3.org/TR/"><abbr title="World Wide Web Consortium">W3C</abbr> technical reports index</a> at
      https://www.w3.org/TR/.</em></p>
	  <p>
    This document was published by the <a href="https://www.w3.org/WAI/APA/">Accessible Platform Architectures Working Group</a> as an
    Editor's Draft. 
    
  </p><p>
    <a href="https://github.com/w3c/apa/issues/">GitHub Issues</a> are preferred for
          discussion of this specification.
        
    
  </p><p>
      Publication as an Editor's Draft does not imply endorsement
      by the <abbr title="World Wide Web Consortium">W3C</abbr> Membership. 
    </p><p>This is a draft document and may be updated, replaced
  or obsoleted by other documents at any time. It is inappropriate to cite this
  document as other than work in progress.
  </p><p></p><p>
    
        This document was produced by a group
        operating under the
        <a href="https://www.w3.org/Consortium/Patent-Policy/"><abbr title="World Wide Web Consortium">W3C</abbr> Patent
          Policy</a>.
       The group does not expect this document to become a <abbr title="World Wide Web Consortium">W3C</abbr> Recommendation.
    
                <abbr title="World Wide Web Consortium">W3C</abbr> maintains a
                <a rel="disclosure" href="https://www.w3.org/groups/wg/apa/ipr">public list of any patent disclosures</a>
          made in connection with the deliverables of
          the group; that page also includes
          instructions for disclosing a patent. An individual who has actual
          knowledge of a patent which the individual believes contains
          <a href="https://www.w3.org/Consortium/Patent-Policy/#def-essential">Essential Claim(s)</a>
          must disclose the information in accordance with
          <a href="https://www.w3.org/Consortium/Patent-Policy/#sec-Disclosure">section 6 of the <abbr title="World Wide Web Consortium">W3C</abbr> Patent Policy</a>.
        
    
  </p><p>
                  This document is governed by the
                  <a id="w3c_process_revision" href="https://www.w3.org/2020/Process-20200915/">15 September 2020 <abbr title="World Wide Web Consortium">W3C</abbr> Process Document</a>.
                </p></section><nav id="toc"><h2 class="introductory" id="table-of-contents">Table of Contents</h2><ol class="toc"><li class="tocline"><a class="tocxref" href="#abstract">Abstract</a></li><li class="tocline"><a class="tocxref" href="#sotd">Status of This Document</a></li><li class="tocline"><a class="tocxref" href="#introduction"><bdi class="secno">1. </bdi>Introduction</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#media-synchronization"><bdi class="secno">1.1 </bdi>Media Synchronization</a></li><li class="tocline"><a class="tocxref" href="#media-synchronization-and-accessibility"><bdi class="secno">1.2 </bdi>Media Synchronization and Accessibility</a></li><li class="tocline"><a class="tocxref" href="#associated-publications"><bdi class="secno">1.3 </bdi>Associated Publications</a></li></ol></li><li class="tocline"><a class="tocxref" href="#issues-and-opportunities-identified-in-the-literature"><bdi class="secno">2. </bdi>Issues and Opportunities Identified in the Literature</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#lip-reading-use-case-synchronization"><bdi class="secno">2.1 </bdi>Lip Reading Use Case Synchronization</a></li><li class="tocline"><a class="tocxref" href="#caption-synchronization"><bdi class="secno">2.2 </bdi>Caption Synchronization</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#caption-rate"><bdi class="secno">2.2.1 </bdi>Caption Rate</a></li><li class="tocline"><a class="tocxref" href="#captions-in-live-media"><bdi class="secno">2.2.2 </bdi>Captions in Live Media</a></li></ol></li><li class="tocline"><a class="tocxref" href="#sign-language-interpretation-synchronization"><bdi class="secno">2.3 </bdi>Sign Language Interpretation Synchronization</a></li><li class="tocline"><a class="tocxref" href="#video-description-synchronization"><bdi class="secno">2.4 </bdi>Video Description Synchronization</a></li><li class="tocline"><a class="tocxref" href="#xr-environment-synchronization"><bdi class="secno">2.5 </bdi>XR Environment Synchronization</a></li></ol></li><li class="tocline"><a class="tocxref" href="#references-0"><bdi class="secno">3. </bdi>References</a></li><li class="tocline"><a class="tocxref" href="#references"><bdi class="secno">A. </bdi>References</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#informative-references"><bdi class="secno">A.1 </bdi>Informative references</a></li></ol></li></ol></nav>
	  <section id="introduction">
	    <h2 id="x1-introduction"><bdi class="secno">1. </bdi>Introduction<a class="self-link" aria-label="§" href="#introduction"></a></h2>
	    <section id="media-synchronization">
	      <h3 id="x1-1-media-synchronization"><bdi class="secno">1.1 </bdi>Media Synchronization<a class="self-link" aria-label="§" href="#media-synchronization"></a></h3>
	      <p>In accessible multimedia content, a variety of resources may be presented concurrently. These resources can include a video track, an audio track, captions, video descriptions, and sign language interpretation of the audio track. To ensure equality of access for all users, including those with a variety of disabilities and associated needs, these concurrent resources should be appropriately synchronized. For example, adequate synchronization of the audio and video tracks is necessary to support users who are hard of hearing, and who rely on lip reading to understand spoken content. Users who have difficulty hearing for situational reasons (e.g., due to a noisy environment) also benefit.</p>
	      <p>This document addresses the question of what qualifies as sufficient synchronization of the different media resources that may be used in accessible content. The considerations that bear on this question are different depending on the resources involved (audio and video tracks, captions, sign language interpretation, etc.). Likewise, the applicable constraints vary according to whether the multimedia content is presented in real time (as in a video conference or a live event), or prerecorded.</p>
				<div class="note" id="issue-container-generatedID"><div role="heading" class="ednote-title marker" id="h-ednote" aria-level="4"><span>Editor's note</span></div><aside class="">
					<p>The Task Force is considering whether to include guidance for implementers in this document. Such guidance could be differentiated according to whether the media are live or prerecorded. For example, synchronization tolerances achievable in real-time communication contexts are different from what can be achieved in a prerecorded video.</p>
					<p>Comments on whether such guidance should be included, and whether it should be organized by the type of media (live/prerecorded) are sought. These review comments will assist the Task Force in deciding how best to summarize the research findings considered in this draft, and what, if any, guidance to provide.</p>
	</aside></div>
	      <p>Adequate synchronization benefits all users. Consequently, the research surveyed in this document is of general importance to the quality of multimedia for all user populations. It is especially significant, however, to users with disabilities who need alternative media resources, such as captions, descriptions, and sign language interpretation.</p>
	    </section>
	    <section id="media-synchronization-and-accessibility">
	      <h3 id="x1-2-media-synchronization-and-accessibility"><bdi class="secno">1.2 </bdi>Media Synchronization and Accessibility<a class="self-link" aria-label="§" href="#media-synchronization-and-accessibility"></a></h3>
	      <p>The adequacy of media synchronization can significantly affect the accessibility of content. For example, for a person who uses captions to follow the progression of a video successfully, a correspondence should be maintained between the captions and the visual track (both of which the user is watching concurrently). This can be accomplished by limiting the delay between the spoken dialogue and the presentation of the captions. The issue of what delay should be regarded as acceptable in such a case is addressed in this document with respect to a variety of media resources.</p>
	      <p>More precisely, one media track can be ahead of or behind another media track by a specific time interval. The problem of adequate synchronization can be understood as that of defining the appropriate tolerances for the relationship between different kinds of media resources, such as audio and video tracks. The purpose of limiting the acceptable time window is to facilitate comprehension of the material. Insufficient synchronization leads to a corresponding loss in comprehension.</p>
	      <p>Issues of media synchronization are relevant to multiple aspects of Web technology. These aspects include the design and implementation of Web standards for media synchronization (e.g., Timed Text), the authoring tools with which accessible multimedia content is created, and the Web applications through which it is presented to the user. This document can serve as a point of reference for the development of each of these technologies. It can also inform the further evolution of standards for Web accessibility, and indeed for multimedia accessibility in general.</p>
	    </section>
	    <section id="associated-publications">
	      <h3 id="x1-3-associated-publications"><bdi class="secno">1.3 </bdi>Associated Publications<a class="self-link" aria-label="§" href="#associated-publications"></a></h3>
	      <p>This document is closely related to other publications developed by the <abbr title="World Wide Web Consortium">W3C</abbr>'s Web Accessibility Initiative. Normative guidance concerning the accessibility of multimedia is given in <cite>Web Content Accessibility Guidelines (WCAG)</cite> 2.1 [<cite><a class="bibref" data-link-type="biblio" href="#bib-wcag21" title="Web Content Accessibility Guidelines (WCAG) 2.1">wcag21</a></cite>]. Detailed, non-normative guidance to the accessibility-related aspects of multimedia content is presented in the <cite>Media Accessibility User Requirements (MAUR)</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-media-accessibility-reqs" title="Media Accessibility User Requirements">media-accessibility-reqs</a></cite>], a document that identifies users' needs and associated solutions.</p>
	      <p>Synchronized media can occur in immersive environments, including virtual reality and augmented reality. The <cite>XR Accessibility User Requirements (XAUR)</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-xaur" title="XR Accessibility User Requirements">xaur</a></cite>] should be consulted for guidance concerning the accessibility of these technologies. Similarly, synchronized media can arise in real-time communication applications, such as remote meeting environments. The accessibility-related user needs and associated system requirements applicable to these applications are considered in <cite>RTC Accessibility User Requirements (RAUR)</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-raur" title="RTC Accessibility User Requirements">raur</a></cite>]. The present document should be regarded as complementing each of these publications by examining a specific aspect of media quality and accessibility.</p>
	    </section>
	  </section>
<section id="issues-and-opportunities-identified-in-the-literature">
<h2 id="x2-issues-and-opportunities-identified-in-the-literature"><bdi class="secno">2. </bdi>Issues and Opportunities Identified in the Literature<a class="self-link" aria-label="§" href="#issues-and-opportunities-identified-in-the-literature"></a></h2>
<section id="lip-reading-use-case-synchronization">
<h3 id="x2-1-lip-reading-use-case-synchronization"><bdi class="secno">2.1 </bdi>Lip Reading Use Case Synchronization<a class="self-link" aria-label="§" href="#lip-reading-use-case-synchronization"></a></h3>
<p>Research in the field of human speech perception underscores the fact that speech perception is routinely bimodal in nature, and depends not only on acoustic cues in human speech but also on visual cues such as lip movements and facial expressions. Due to this bimodality in speech perception, audio-visual interaction becomes an important design factor for multimodal communication systems, such as video telephony and video conferencing. (Chen &amp; Rao, 1998)</p>
<p>It has been observed that humans use their sight to assist in aural communication. This has been found to be especially true in helping to separate speech from background noise by supplying a supplemental visual information source, which is useful when the listener has trouble comprehending the acoustic speech. Past research has shown that in such situations, even people who are not hard of hearing depend upon such visual cues to some extent (Summerfield, 1992). Access to robust visual speech information has been shown to lead to significant improvement in speech recognition in noisy environments. For instance, one study found that when only acoustic access to speech was available, auditory recognition was near 100% at 0 dB of signal-to-noise ratio (SNR) but fell to under 20% at minus 30 dB SNR. However, this study found that when visual access to the speaker was included, recognition only dropped from 100% to 90% over the same range (Sumby &amp; Pollack, 1954). More recent studies have shown that when sighted people are attempting to listen to speech in high noise audiovisual samples, the result is greater visual fixations on the mouth of the speaker (Yi, Wong, &amp; Eizenman, 2013) and stronger synchronizations between the auditory and visual motion/motor brain regions (Alho et al., 2014).</p>
<p>A similar reliance on visual cues to help decode speech may also be at work in other instances where volume of the speaker's voice begins to degrade, such as while listening to a lecture in a large hall. Due to the fact that light travels at a much higher speed than sound, in a face-to-face setting a person will see a speaker’s lips and facial gestures sooner than the sound of the speaker’s voice arrives. In a normal in-person conversation this difference is negligible. However, as the distance increases, such as a student listening to an instructor in the classroom, this time lag will increase. For instance, at 22 feet, this difference is roughly 20 ms. At the same time, the listener’s perceived volume of a speakers voice drops with the distance traveled, which means the listener will rely more on visual cues. Indeed, experimental research has demonstrated that the ability to comprehend speech at increasing distances is improved when both audio and visual speech is available to the listener (Jordan &amp; Sergeant, 2000). Such findings suggest that robust synchronized video along with the audio of speakers in virtual environments are likely to increase speech comprehension for hard of hearing listeners.</p>
<p>One important concern in audiovisual integration of speech audio and visual information is how closely these events are synchronized. Given that the observable facial movement for phoneme production can precede acoustic information by 100–200 ms, the temporal order of both the sensory input and electrophysiological effects suggests that visual speech information may provide predictions about upcoming auditory input. This fact likely explains why research has found that test subjects are less likely to notice minor auditory lags in audiovisual presentation of human speech than when the audio signal arrives first (Peelle &amp; Sommers, 2015). As a result, several standards bodied have attempted to set synchronization specifications for audiovisual broadcasting which typically provide a +/- threshold where audio lag is much less restrictive than video lag. Typically, these thresholds are more restrictive for higher quality signals, such as those for digital high definition television broadcasting. Case in point, the recognized industry standard adopted by the ATSC Implementation Subcommittee, the DSL Forum, and the ITU-T Recommendation G.1080, all include an audio/video delay threshold between plus 15 ms and minus 45 ms (Staelens et al, 2012). This means that having the audio arrive up to 45 ms after the video is considered acceptable, but having the audio signal arrive more than 15 ms before the video is objectionable. Consistently with this approach, the EN 301 549 information and communication technology public procurement standard [<cite><a class="bibref" data-link-type="biblio" href="#bib-en-301-549" title="EN 301 549 v3.2.1: Harmonised European Standard - Accessibility requirements for ICT products and services">en-301-549</a></cite>] specifies a maximum time difference of 100 ms between the audio and the video, noting that the decline in intelligibility is greater if the audio is ahead of, rather than behind, the video track.</p>
<p>However, it is important to note that most audiovisual media in everyday life does not meet the capabilities of high definition television. Further, most experimental studies which have attempted to examine issues around lip video synchronization with audio have been conducted with standard video recording capabilities which are typically limited to a frame rate of 25 frames per second (fps). At 25 fps, there will be one frame every 40 ms, and as a result it becomes impossible to test synchronization errors below this time threshold (Ivanko et al, 2018). Studies using high quality audiovisual content at much faster frame rates have shown that lip synchronization mismatch of 20 ms or less is imperceptible (Firestone, 2007). However, studies conducted on speech intelligibility when audio quality is degraded in such a way as to simulate age-related hearing loss have shown that when the audio signal leads the video, intelligibility declines appreciably for even the shortest asynchrony of 40 ms, but when the video signal leads the audio, intelligibility remains relatively stable for onset asynchronies up to 160 - 200 ms (Grant &amp; Greenberg, 2001). These findings suggest that, from an accessibility perspective, the audio signal should not be ahead of the video by more than 40 ms, and the video should not be ahead of the audio by more than 160 ms. However, less than 160 ms offset is desirable due to the fact that this much of a delay would be detectable and potentially objectionable to a percentage of the population, even though it would not present an accessibility barrier as such.</p>
</section>
<section id="caption-synchronization">
<h3 id="x2-2-caption-synchronization"><bdi class="secno">2.2 </bdi>Caption Synchronization<a class="self-link" aria-label="§" href="#caption-synchronization"></a></h3>
<p>Captions used for accessibility purposes (also more commonly known as "subtitles" in some countries) have been in common usage in the broadcast industry for several decades. Some of the critical issues related to captioning which have been examined in research include the caption rate, the quality of caption text (including aspects such as caption text accuracy, verbatim vs. edited captions, identification on multiple speakers, and the use of punctuation and capitalization), as well as the synchronization of caption text with audio and visual information.</p>
<section id="caption-rate">
<h4 id="x2-2-1-caption-rate"><bdi class="secno">2.2.1 </bdi>Caption Rate<a class="self-link" aria-label="§" href="#caption-rate"></a></h4>
<p>Caption rate has been a major topic for the broadcast industry. In a White Paper published by BBC Research &amp; Development (Sandford, 2015), the author summarized the various guidelines in use among broadcasters which often include both optimal and maximum rates for captions. Figures of approximately 140 Words per Minute (WPM) as the optimum subtitle (i.e., caption) rate, and around 180-200 WPM as the maximum rate were found to be common. However, the conclusion of the author was that the guidelines examined "fail to cite research supporting these figures but justify them by stating that above these rates, subtitles will be difficult to follow." Sandford further noted that previous research has shown that reading comprehension of captions remain fairly stable up to at least a rate of 230 WPM, which seemed to call into question the maximum rates used in most guidelines to that point in time, and served as the impetus for new research conducted by the BBC.</p>
<p>The BBC research study on caption rates was conducted in two phases. The first phase of the study included video clips which were purposefully created for the study, where BBC reporters attempted to recreate broadcast quality news pieces on the same topic, but re-scripted each clip in the study so that it included more or less words which were spoken over the same 30 second period of time. In this way, a range of WPM caption rates were created while all other aspects of the clip remained the same, except that they created two types of captions, one with scrolling captions and one with block captions. This series of clips at different rates were then shown to test subjects, which included two main groups. One group of testers included deaf and hard-of-hearing viewers who viewed the video clips with captions (both scrolling and block), while a comparison group of hearing viewers viewed the same series of clips without any captions at all. The purpose of the comparison group of hearing viewers was to help gauge how much of the impact on perceived good and bad rates of captions may be due to how quickly the speaker is talking, as opposed to how fast the words appear in captions.</p>
<p>Their results for this phase of the study showed that the range of rates between what subjects considered "too fast" and "too slow" was widest for block subtitles and narrowest for speech alone. The analysis revealed that:</p>
<ul>
<li>The average rate of clips perceived as "slow" came in at 112 WPM for block captions, 115 WPM for scrolling captions, and 121 WPM for speech alone.</li>
<li>The optimal "good" rate averaged 177 WPM for block captions, 171 for scrolling captions, and 170 WPM for speech alone.</li>
<li>The average rate of clips perceived as "fast" came in at 242 for block captions, 227 for scrolling captions, and 219 for speech alone.</li>
</ul>
<p>However, the researchers concluded that overall similarity between all of the results demonstrated that the WPM rate of the caption text was not an independent factor for the study subjects' perception of rates that are too slow or too fast. Generally speaking, it was found that when the rate of speech was perceived as too fast or too slow by hearing viewers, this same range of rates for caption text was likely to be similarly perceived as too fast or too slow by viewers who were deaf and hard of hearing. Indeed, if anything, this set of data seems to suggest that--at least among the sample of subjects in this study--hearing viewers are more critically attentive to word rates in spoken audio than deaf and hard-of-hearing users are for word rates in captioned text for the same content. Overall, the researchers concluded that, "We found no problems associated with the rate of subtitles when they matched natural speech, regardless of the rate in words per minute."</p>
<p>During the second phase of the BBC study, researchers collected a number of sample clips from eight different examples of television programming "in the wild" which were above a 200 WPM rate, and presented them only to the deaf and hard-of-hearing study subjects. The expectation based on phase one of the study was that these higher rate clips would be more likely to be perceived as faster than optimal. However, the results showed that this was not the case, and that the mean perceived rates for all clips were closer to "good" and well under the "fast" rate than would have been predicted based on the findings of phase one of the study. Nonetheless, there were some telling distinctions in perception ratings based upon the type of programming. One case in point can be made by comparing ratings for two television episode clips which were nearly identical in word rate, but had a relatively wide spread in perception of how close to an optimal "enjoyable" rate, based on study subjects' numeric scores on a Likert scale. In this comparison, a clip from the talk show Top Gear with a rate of 256 WPM received a Likert scale score of 3.40 for an enjoyable rate (where 5 would be considered at the top of the "enjoyable" word rate scale), while a clip the cooking show Kitchen with an almost identical rate of 259 WPM received a lower Likert scale score of 2.34--more than a full point below the Top Gear clip, and lower than any other clip among the eight television episodes reviewed in the study.</p>
<p>While the BBC researchers in this study did not interview subjects to get additional qualitative details on subjects' ratings for individual clips, one likely conclusion is that the perception of a good or enjoyable rate for captions is tied to some extent to the type of content, and the way the viewer may plan to use the information gleaned from watching it. Whereas a typical viewer of Top Gear is likely to be more interested in the general entertainment value of watching the show, someone viewing a cooking show is more likely to be interested in actually using the information by cooking the dish being prepared on the screen. In the latter case, the viewer will often be very interested in specific details about ingredients, amounts and the cooking process. This consideration is likely very important in the context of educational video programming, in particular, where the desire is that students will comprehend and retain key facts from video programming. And although the researchers did not study these "in the wild" samples with hearing viewers, the findings from phase one of their study would logically point back to the underlying issue of speech rate in the audio stream to begin with, and suggests that media producers be careful to regulate the speed of speakers in media materials, especially when the voice content has a high information density.</p>

<p>Another study by the BBC aimed to measure the perceived quality of television captions based on guidelines for measuring perceived audio quality. The objective was to estimate the relative impact of reduced delay in the appearance of live captions vs an increase in accuracy. Participants were regular users of captions, but were not asked to disclose their hearing ability. Reduced delay in the presentation of word-by-word captions was more strongly associated with a perception of improved quality for participants watching with sound on than for those with sound turned off. On the other hand, improvement in caption accuracy (namely, a lower word error rate) was significantly associated with a perception of improved quality only among participants who viewed the material without sound (Armstrong, 2013).</p>

</section>
<section id="captions-in-live-media">
<h4 id="x2-2-2-captions-in-live-media"><bdi class="secno">2.2.2 </bdi>Captions in Live Media<a class="self-link" aria-label="§" href="#captions-in-live-media"></a></h4>
<p>There are three main transcription methods for live captions. ASR (Automatic Speech Recognition), ASR with revoicing, and STTR (Speech To Text Reporting). ASR is a fully automated transcription process where a computer converts the speech into text. ASR with revoicing utilises a human intermediary who repeats everything that is spoken to an ASR system that is trained to their voice. STTR involves specially trained typists phonetically transcribing what is said using chord keyboards. The two main types of these keyboards are Palantype and Stenotype and Stenotype is the most common. Cost and accuracy both increase relative to how much of the work is done by a human.</p>
<p>Captions in live media and remote meetings will be inherently delayed due to the necessary time lag for speech to be transcribed into text captions. Even automated captions produced by ASR systems require some amount of time to process human speech into text which then must be integrated into the video stream. The use of human transcribers to create captions, while typically resulting in captions of much greater accuracy, will usually dictate an even greater latency between the sound of the speaker's voice and the displayed captions. The understanding of what is considered an "acceptable" amount of time delay will often hinge on the type of live media, and what is considered the proper level of transcription accuracy. For instance, in the case of live speeches and broadcast entertainment, media outlets have adopted caption latency standards ranging from a target as short as 3 seconds to as long as "less than 10 seconds" -- the latter case "reflecting a greater emphasis on ensuring that spelling and punctuation are correct" (Mikul, 2014). The conclusion of Mikul is that a target latency of 5 seconds is appropriate and achievable in most cases for live broadcast media, and that this target applies to the average time lag over the length of the program.</p>
<p>However, caption time lag in remote meetings must be considered in a different light, as the participatory nature of meetings dictate that the immediacy of captioned text must take some degree of precedence over spelling and punctuation accuracy. While both accuracy and immediacy are vital criteria in any setting, having captions delayed for an inordinate amount of time during a remote meeting scenario puts the deaf or hard of hearing meeting participant at a significant disadvantage during a fast-moving discussion. To better address the need for immediacy of captions in remote meetings, most popular online meeting platforms have integrated automatic captioning utilizing Automatic Speech Recognition (ASR).</p>
<p>The accuracy of ASR can vary greatly due to the influence of factors that tend to introduce recognition errors. These factors include speaker variability, reflecting for example illness, fatigue or emotional state, differences of dialect and accent, as well as discrepancies between the audio characteristics of the speech samples used to train the system and the speech which is to be recognized, such as the presence of background noise (Errattahi, El Hannani, &amp; Ouahmane, 2018). Nevertheless, the accuracy of ASR systems has markedly improved in recent years. Indeed, very recent studies have demonstrated that, under favorable conditions, the best ASR systems can rival human accuracy on the average while also decreasing captioning latency to well below the typical human captioning ability. A 2020 study comparing ASR-based captioning systems revealed that the Google enhanced API had a stable-hypothesis latency (the time between the utterance of a word and the output of correct text) of only 0.761 seconds, while maintaining a Word Error Rate (WER) of only 0.06 (Jiline et al, 2020). The authors then compared this to an average latency of 4.2 seconds for human based captioning and a WER between 0.04 and 0.09, based on generalized results from multiple academic sources. While Google's enhanced ASR API was by far the best in this study comparison, its performance illustrates the growing capability of machine learning to enhance the ability of remote meeting platforms to provide accurate captions in a timely manner.</p>
<div class="note" id="issue-container-generatedID-0"><div role="heading" class="ednote-title marker" id="h-ednote-0" aria-level="5"><span>Editor's note</span></div><div class="">
  <p>The Task Force plans to provide here a more adequate characterization of the trade-off between caption latency and the accuracy of automatic speech recognition systems, compared with that of human transcription. Review and comments on this point are invited.</p>
  <p>The Task Force is also aware of recent decisions by the U.S. Federal Communications Commission granting conditional certification for ASR generated captions in Internet telephony. See Federal Communications Commission (2018) for the legal background to these decisions. This and similar research or regulatory activities will be of interest for subsequent drafts of this document.</p>
</div></div>
</section>
</section>
<section id="sign-language-interpretation-synchronization">
<h3 id="x2-3-sign-language-interpretation-synchronization"><bdi class="secno">2.3 </bdi>Sign Language Interpretation Synchronization<a class="self-link" aria-label="§" href="#sign-language-interpretation-synchronization"></a></h3>
<p>While the use of closed captions in both live and prerecorded video has become widespread, the use of a human signer to provide interpretation of spoken content in media is not nearly as prevalent. In some cases, broadcasters have argued that captioning is more cost effective and reaches a larger audience of users, such as hard-of-hearing and late-deafened individuals who are not literate in sign language. However, the Deaf community has long advocated for increased availability to sign language interpretation as better meeting their access needs (Bosch-Baliarda, Soler-Vilageliu &amp; Orero 2020). And while significant research and development work has been directed toward automated sign language translation using computer-generated signing avatars, this work is still behind the current state of automated speech recognition captioning technology (Bragg et al, 2019).</p>
<p>Due to the fact that sign languages have their own grammars which are not necessarily aligned to the written form of the associated spoken language, it is not possible to provide a word-by-word rendering as is done with captioning, and thus uniform synchronization with spoken audio will not be possible. Indeed, in practice a sign language interpreter will often need to wait for some few seconds to allow for an understanding of more complete spoken phrasing before starting to interpret in sign. The amount of onset time lag may vary widely depending upon the particular spoken language and the particular target sign language source.</p>
<p>In a 1983 study by Cokely, researchers found that an increased lag time actually enhanced the overall comprehension of the spoken dialogue and allowed the sign language interpreter to convey a more accurate rendering of what was spoken (Cokely, 1986). In this study, it was found that the number of translation errors (i.e., various types of translation miscues) decreases as the lag time of the interpreters increases. For examples, the interpreters in their study with a 2-second lag time had more than twice the total number of miscues of the interpreters with a 4-second lag, who in turn had almost twice as many miscues as those with a 6-second lag. The researchers cautioned, however, that this does not mean there is no upper limit to lag time and reasoned that it is likely there is lag time threshold beyond which the number of translation omissions would significantly increase because the threshold is at the upper limits of the individual's short-term working memory. Nonetheless, the findings of this study point out that providing close synchronization of sign language interpretation to what is being spoken may be counterproductive. In this case, some users may prefer finding a happy medium between the user need for immediacy in remote meetings and the user need for accuracy, while others may prefer the greatest accuracy possible even at the expense of immediacy.</p>

</section>
<section id="video-description-synchronization">
<h3 id="x2-4-video-description-synchronization"><bdi class="secno">2.4 </bdi>Video Description Synchronization<a class="self-link" aria-label="§" href="#video-description-synchronization"></a></h3>
<p>Video description (sometimes referred to as “audio description”) typically adds spoken narration of important visual elements in video streams such as TV programs and movies. Beginning in the early 1990s, the ability to transmit and receive audio descriptions in TV programming over a Separate Audio Program (SAP) channel became available (Cronin &amp; King, 1990). Video description is most commonly applied to prerecorded media, although its application to live events (especially the performing arts) is also a common use case (Di Giovanni, 2018).</p> 
<div class="note" role="note" id="issue-container-generatedID-1"><div role="heading" class="note-title marker" id="h-note" aria-level="4"><span>Note</span></div><aside class="">
  <p>Video description can be delivered by two alternative means:</p>
  <ul>
    <li>As an audio track synchronized with the visual track, or</li>
    <li>as text for presentation to the user via a text to speech system or a braille device. This is referred to in the <cite>Media Accessibility User Requirements</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-media-accessibility-reqs" title="Media Accessibility User Requirements">media-accessibility-reqs</a></cite>] as "video text description".</li>
  </ul>
  <p>Whereas the first option requires synchronization of an audio track with the video, the second option is dependent for synchronization on the user's preferences (e.g., speech rate settings). Only the start time of the cue for each description is supplied by the media provider; the end time varies according to the user's local preferences, and may necessitate automatic pausing of the video and audio tracks of the media resource to accommodate the reading of a description. See <cite>Media Accessibility User Requirements</cite> [<cite><a class="bibref" data-link-type="biblio" href="#bib-media-accessibility-reqs" title="Media Accessibility User Requirements">media-accessibility-reqs</a></cite>], section 2.2, for further details.</p>
  <p>Since control of synchronization resides with the creator of the media resource if video description is provided as a supplemental audio track, only this case is considered further in the discussion that follows. The synchronization of video text description can be addressed by an appropriate implementation of client-side software that respects the user's preferences (including speech rate for text to speech systems, and scrolling behavior for braille devices).</p>
</aside></div>
<p>One of the most difficult considerations of video description creation is the need to avoid conflicts with the primary speech dialogue. In prerecorded media, a complete transcript of the spoken dialogue with timings is commonly loaded into video description editing software, although in some cases a simple spreadsheet may be used for smaller projects. The next step is typically the identification and inclusion of onscreen events, music and sound effect cues in the media time stream. Editing software may apply an algorithm that calculates the ideal duration of the description entered into the available open space time slot, as well as the minimum and maximum tolerated deviation from the ideal reading rate. In order to do that, the algorithm needs to be given reading rate values upon which it can calculate (Jankowska et al, 2017). In practice, the description of an on-scene event may need to begin several seconds before the event occurs to avoid audio conflicts.</p>
<p>Live events, however, are more difficult to manage than prerecorded media due to the spontaneity of an event happening in real time. While video description can often be scripted during rehearsals of performances and thus made available in real time during a performance, adding video description to a live event which is not rehearsed is much more difficult because there is no pre-event information as to the availability and duration of open slots in the live audio stream. One method used to address this scenario in broadcasting of live evens is “near real-time” video description (Boyce, et al, ND). In near real-time broadcasting, the live event is recorded and transmission is typically delayed within the range of 10 to 60 seconds. This allows time for the system to look ahead and analyze the upcoming portion of the video for silent periods and provides a brief span of time for the describer to insert the narration. While near real-time video description may work well in mainstream media broadcasts, it would be impractical for online participatory events, meetings and group discussions. In such cases, the generally accepted best practice is for participants to always describe visual aspects of content or on-camera actions they may be demonstrating as closely in sync as possible with the visual information.</p> 
</section>
<section id="xr-environment-synchronization">
<h3 id="x2-5-xr-environment-synchronization"><bdi class="secno">2.5 </bdi>XR Environment Synchronization<a class="self-link" aria-label="§" href="#xr-environment-synchronization"></a></h3>
<div class="note" id="issue-container-generatedID-2"><div role="heading" class="ednote-title marker" id="h-ednote-1" aria-level="4"><span>Editor's note</span></div><p class="">Do users' needs, and the acceptable synchronization tolerances applicable to immersive environments (e.g., virtual reality, augmented reality, and 360-degree video) differ from what is encountered in multimedia in general? What research, if any, has been undertaken into such differences between synchronization in immersive and non-immersive media? If there are specific synchronization issues relevant to immersive environments, they will be documented in this section. The Research Questions Task Force and the Accessible Platform Architectures working Group invite comments regarding these issues to inform further development of the draft.</p></div>
</section>
</section>
<section id="references-0">
<h2 id="x3-references"><bdi class="secno">3. </bdi>References<a class="self-link" aria-label="§" href="#references-0"></a></h2>
<ul>
<li>Alho, J., Lin, F. H., Sato, M., Tiitinen, H., Sams, M., &amp; Jääskeläinen, I. P. (2014). Enhanced neural synchrony between left auditory and premotor cortex is associated with successful phonetic categorization. Frontiers in Psychology, 5, 394.</li>
<li>Armstrong, M. (Oct 2013). The Development of a Methodology to Evaluate the Perceived Quality of Live TV Subtitles. BBC Research &amp; Development White Paper WHP 259 </li>
<li>Berke, L., Caulfield, C., &amp; Huenerfauth, M. (2017, October). Deaf and hard-of-hearing perspectives on imperfect automatic speech recognition for captioning one-on-one meetings. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (pp. 155-164).</li>
<li>Blakowski, G., &amp; Steinmetz, R. (1996). A media synchronization survey: Reference model, specification, and case studies. IEEE journal on selected areas in communications, 14(1), 5-35.</li>
<li>Bosch-Baliarda, Marta; Olga Soler-Vilageliu &amp; Pilar Orero. (2020) “Sign language interpreting on TV: a reception study of visual screen exploration in deaf signing users.” In: Richart-Marset, Mabel &amp; Francesca Calamita (eds.) 2020. Traducción y Accesibilidad en los medios de comunicación: de la teoría a la práctica / Translation and Media Accessibility: from Theory to Practice. MonTI 12, pp. 108-143. Esta obra está bajo una licencia de Creative Commons Reconocimiento 4.0 Internacional.</li>
<li>Boyce, M., Diamond, S., Fels, D., Gadsby, E., Harvie, R., Porch, W., ... &amp; Treviranus, J. (N.D.). Canadian Network for Inclusive Cultural Exchange (CNICE).</li>
<li>Bragg, D., Koller, O., Bellard, M., Berke, L., Boudreault, P., Braffort, A., ... &amp; Ringel Morris, M. (2019, October). Sign language recognition, generation, and translation: An interdisciplinary perspective. In The 21st International ACM SIGACCESS Conference on Computers and Accessibility (pp. 16-31).</li>
<li>Burnham, D., Robert-Ribes, J., &amp; Ellison, R. (1998). Why captions have to be on time. In AVSP'98 International Conference on Auditory-Visual Speech Processing.</li>
<li>Chen, M. (2003, April). A low-latency lip-synchronized videoconferencing system. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 465-471).</li>
<li>Chen, T., &amp; Rao, R. R. (1998). Audio-visual integration in multimodal communication. Proceedings of the IEEE, 86(5), 837-852.</li>
<li>Cokely, D. (1986). The effects of lag time on interpreter errors. Sign Language Studies, 341-375.</li>
<li>Cronin, B. J., &amp; King, S. R. (1990). The Development of the Descriptive Video Services. Journal of Visual Impairment &amp; Blindness, 84(10), 503-506.</li>
<li>Cuzco-Calle, I., Ingavélez-Guerra, P., Robles-Bykbaev, V., &amp; Calle-López, D. (2018, August). An interactive system to automatically generate video summaries and perform subtitles synchronization for persons with hearing loss. In 2018 IEEE XXV International Conference on Electronics, Electrical Engineering and Computing (INTERCON) (pp. 1-4). IEEE.</li>
<li>De Araújo, T. M. U., Ferreira, F. L., Silva, D. A., Oliveira, L. D., Falcão, E. L., Domingues, L. A., ... &amp; Duarte, A. N. (2014). An approach to generate and embed sign language video tracks into multimedia contents. Information Sciences, 281, 762-780</li>
<li>Díaz-Cintas, J., Orero, P., &amp; Remael, A. (Eds.). (2007). Media for all: subtitling for the deaf, video description, and sign language (Vol. 30). Rodopi.</li>
<li>Di Giovanni, E. (2018). Audio description for live performances and audience participation. Jostrans: the Journal of Specialised Translation, 29, 189-211.</li>
<li>Errattahi, R., El Hannani, A., &amp; Ouahmane, H. (2018). Automatic speech recognition errors detection and correction: A review. Procedia Computer Science, 128, 32-37.</li>
<li>Federal Communications Commission. (2018). Declaratory Ruling on Automatic Speech Recognition. In <a href="https://docs.fcc.gov/public/attachments/FCC-18-79A1.docx">Report and Order, Declaratory Ruling, Further Notice of Proposed Rulemaking, and Notice of Inquiry</a> (33 FCC Rcd 5800 (9), 5827, para. 48).</li>
<li>Firestone, S. (2007). Lip Synchronization in Video Conferencing. Voice and Video Conferencing Fundamentals. Cisco Systems, Inc.</li>
<li>Garcia, J. E., Ortega, A., Lleida, E., Lozano, T., Bernues, E., &amp; Sanchez, D. (2009, May). Audio and text synchronization for TV news subtitling based on automatic speech recognition. In 2009 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (pp. 1-6). IEEE.</li>
<li>Grant, K. W., &amp; Greenberg, S. (2001). Speech intelligibility derived from asynchronous processing of auditory-visual information. In AVSP 2001-International Conference on Auditory-Visual Speech Processing.</li>
<li>Han, H. H., &amp; Yu, H. N. (2020). An empirical study of temporal variables and their correlations in spoken and sign language relay interpreting. Babel, 66(4-5), 619-635</li>
<li>Huang, C. W., Hsu, W., &amp; Chang, S. F. (2003). Automatic closed caption alignment based on speech recognition transcripts. Rapport technique, Columbia.</li>
<li>ITU-T, S. H. (1999). Application profile–Sign language and lip-reading real-time conversation using low bit-rate video communication. CCITT Recommendations.</li>
<li>Ivanko, D., Karpov, A., Fedotov, D., Kipyatkova, I., Ryumin, D., Ivanko, D., ... &amp; Zelezny, M. (2018). Multimodal speech recognition: increasing accuracy using high speed video data. Journal on Multimodal User Interfaces, 12(4), 319-328.</li>
<li>Jankowska, A., ZióŁko, B., Igras-Cybulska, M., &amp; Psiuk, A. (2017). Reading rate in filmic audio description. Rivista Internazionale di Tecnica della Traduzione= International Journal of Translation, 19.</li>
<li>Jiline, M., Kirk, D., Quirk, K., Sandler, M., &amp; Monette, M. (2020) A Review Of State-Of-The-Art Automatic Speech Recognition Services For CART And CC Applications. Proceedings of the 2020 NAB Broadcast Engineering and Information Technology (BEIT) Conference, © 2020 National Association of Broadcasters, 1 M Street SE, Washington, DC 20003 USA.</li>
<li>Jordan, T. R., &amp; Sergeant, P. (2000). Effects of distance on visual and audiovisual speech recognition. Language and Speech, 43(1), 107-124.</li>
<li>Kafle, S., &amp; Huenerfauth, M. (2017, October). Evaluating the usability of automatically generated captions for people who are deaf or hard of hearing. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (pp. 165-174).</li>
<li>Kaganovich, N., Schumaker, J., &amp; Rowland, C. (2016). Matching heard and seen speech: An ERP study of audiovisual word recognition. Brain and language, 157-158, 14–24. <a href="https://doi.org/10.1016/j.bandl.2016.04.010">https://doi.org/10.1016/j.bandl.2016.04.010</a></li>
<li>Keating, E., &amp; Mirus, G. (2003). American Sign Language in virtual space: Interactions between deaf users of computer-mediated video communication and the impact of technology on language practices. Language in Society, 693-714.</li>
<li>Koller, O. (2020). Quantitative survey of the state of the art in sign language recognition. arXiv preprint arXiv:2008.09918.</li>
<li>Kumar, P. J., Hu, W., &amp; Yung, Y. (2017). Virtual Reality Based 3D Video Games and Speech-Lip Synchronization Superseding Algebraic Code Excited Linear Prediction. International Journal of Computer and Information Sciences, 4(12), 303-321.</li>
<li>Maruyama, I., Abe, Y., Sawamura, E., Mitsuhashi, T., Ehara, T., &amp; Shirai, K. (1999). Cognitive experiments on timing lag for superimposing closed captions. In Sixth European Conference on Speech Communication and Technology.</li>
<li>McCarthy, J. E., &amp; Swierenga, S. J. (2010). What we know about dyslexia and web accessibility: a research review. Universal Access in the Information Society, 9(2), 147-152.</li>
<li>Mikul, C. (2014). Caption quality: Approaches to standards and measurement. Media Access Australia.</li>
<li>Montagud, M., Cesar, P., Boronat, F., &amp; Jansen, J. (2018). Introduction to media synchronization (MediaSync). In MediaSync (pp. 3-31). Springer, Cham.</li>
<li>Peelle, J. E., &amp; Sommers, M. S. (2015). Prediction and constraint in audiovisual speech perception. Cortex; a journal devoted to the study of the nervous system and behavior, 68, 169–181. <a href="https://doi.org/10.1016/j.cortex.2015.03.006">https://doi.org/10.1016/j.cortex.2015.03.006</a></li>
<li>Petrie, H. L., Weber, G., &amp; Fisher, W. (2005). Personalization, interaction, and navigation in rich multimedia documents for print-disabled users. IBM Systems Journal, 44(3), 629-635.</li>
<li>Piety, P. J. (2004). The language system of audio description: an investigation as a discursive process. Journal of Visual Impairment &amp; Blindness, 98(8), 453-469.</li>
<li>Sandford, J. (2015). The impact of subtitle display rate on enjoyment under normal television viewing conditions. BBC Research &amp; Development White Paper WHP 306.</li>
<li>Shroyer, E. H., &amp; Birch, J. (1980). Captions and reading rates of hearing-impaired students. American Annals of the Deaf, 125(7), 916-922.</li>
<li>Staelens, Nicolas &amp; De Meulenaere, Jonas &amp; Bleumers, Lizzy &amp; Wallendael, Glenn &amp; De Cock, Jan &amp; Geeraert, Koen &amp; Vercammen, Nick &amp; Van den Broeck, Wendy &amp; Vermeulen, Brecht &amp; Van de Walle, Rik &amp; Demeester, Piet. (2012). Assessing the importance of audio/video synchronization for simultaneous translation of video sequences. Multimedia Systems. 18. 10.1007/s00530-012-0262-4.</li>
<li>Sumby, W. H., &amp; Pollack, I. (1954). Visual contribution to speech intelligibility in noise. The journal of the acoustical society of america, 26(2), 212-215.</li>
<li>Summerfield, Q. (1992). Lipreading and audio-visual speech perception. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 335(1273), 71-78.</li>
<li>Venezia, J.H., Thurman, S.M., Matchin, W. et al. Timing in audiovisual speech perception: A mini review and new psychophysical data. Attention, Perception, &amp; Psychophysics, 78, 583–601 (2016). <a href="https://doi.org/10.3758/s13414-015-1026-y">https://doi.org/10.3758/s13414-015-1026-y</a></li>
<li>Waters, K., &amp; Levergood, T. (1994, October). An automatic lip-synchronization algorithm for synthetic faces. In Proceedings of The second ACM international conference on Multimedia (pp. 149-156).</li>
<li>Yi, A., Wong, W., &amp; Eizenman, M. (2013). Gaze patterns and audiovisual speech enhancement. Journal of Speech, Language, and Hearing Research.</li>
<li>Ziegler, C., Keimel, C., Ramdhany, R., &amp; Vinayagamoorthy, V. (2017, June). On time or not on time: A user study on delays in a synchronised companion-screen experience. In Proceedings of the 2017 ACM International Conference on Interactive Experiences for TV and Online Video (pp. 105-114).</li>
</ul>
</section>


<section id="references" class="appendix"><h2 id="a-references"><bdi class="secno">A. </bdi>References<a class="self-link" aria-label="§" href="#references"></a></h2><section id="informative-references">
    <h3 id="a-1-informative-references"><bdi class="secno">A.1 </bdi>Informative references<a class="self-link" aria-label="§" href="#informative-references"></a></h3>
    <dl class="bibliography"><dt id="bib-en-301-549">[en-301-549]</dt><dd><a href="https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf"><cite>EN 301 549 v3.2.1: Harmonised European Standard - Accessibility requirements for ICT products and services</cite></a>.  CEN/CENELEC/ETSI. 2021-03. URL: <a href="https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf">https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf</a></dd><dt id="bib-media-accessibility-reqs">[media-accessibility-reqs]</dt><dd><a href="https://www.w3.org/TR/media-accessibility-reqs/"><cite>Media Accessibility User Requirements</cite></a>. Shane McCarron; Michael Cooper; Mark Sadecki.  W3C. 3 December 2015. W3C Note. URL: <a href="https://www.w3.org/TR/media-accessibility-reqs/">https://www.w3.org/TR/media-accessibility-reqs/</a></dd><dt id="bib-raur">[raur]</dt><dd><a href="https://www.w3.org/TR/raur/"><cite>RTC Accessibility User Requirements</cite></a>. Joshue O'Connor; Janina Sajka; Jason White; Michael Cooper.  W3C. 25 May 2021. W3C Note. URL: <a href="https://www.w3.org/TR/raur/">https://www.w3.org/TR/raur/</a></dd><dt id="bib-wcag21">[wcag21]</dt><dd><a href="https://www.w3.org/TR/WCAG21/"><cite>Web Content Accessibility Guidelines (WCAG) 2.1</cite></a>. Andrew Kirkpatrick; Joshue O'Connor; Alastair Campbell; Michael Cooper.  W3C. 5 June 2018. W3C Recommendation. URL: <a href="https://www.w3.org/TR/WCAG21/">https://www.w3.org/TR/WCAG21/</a></dd><dt id="bib-xaur">[xaur]</dt><dd><a href="https://www.w3.org/TR/xaur/"><cite>XR Accessibility User Requirements</cite></a>.  W3C. 16 Sept 2020. URL: <a href="https://www.w3.org/TR/xaur/">https://www.w3.org/TR/xaur/</a></dd></dl>
  </section></section><p role="navigation" id="back-to-top">
    <a href="#title"><abbr title="Back to Top">↑</abbr></a>
  </p><script id="respec-dfn-panel">(() => {
// @ts-check
if (document.respec) {
  document.respec.ready.then(setupPanel);
} else {
  setupPanel();
}

function setupPanel() {
  const listener = panelListener();
  document.body.addEventListener("keydown", listener);
  document.body.addEventListener("click", listener);
}

function panelListener() {
  /** @type {HTMLElement} */
  let panel = null;
  return event => {
    const { target, type } = event;

    if (!(target instanceof HTMLElement)) return;

    // For keys, we only care about Enter key to activate the panel
    // otherwise it's activated via a click.
    if (type === "keydown" && event.key !== "Enter") return;

    const action = deriveAction(event);

    switch (action) {
      case "show": {
        hidePanel(panel);
        /** @type {HTMLElement} */
        const dfn = target.closest("dfn, .index-term");
        panel = document.getElementById(`dfn-panel-for-${dfn.id}`);
        const coords = deriveCoordinates(event);
        displayPanel(dfn, panel, coords);
        break;
      }
      case "dock": {
        panel.style.left = null;
        panel.style.top = null;
        panel.classList.add("docked");
        break;
      }
      case "hide": {
        hidePanel(panel);
        panel = null;
        break;
      }
    }
  };
}

/**
 * @param {MouseEvent|KeyboardEvent} event
 */
function deriveCoordinates(event) {
  const target = /** @type HTMLElement */ (event.target);

  // We prevent synthetic AT clicks from putting
  // the dialog in a weird place. The AT events sometimes
  // lack coordinates, so they have clientX/Y = 0
  const rect = target.getBoundingClientRect();
  if (
    event instanceof MouseEvent &&
    event.clientX >= rect.left &&
    event.clientY >= rect.top
  ) {
    // The event probably happened inside the bounding rect...
    return { x: event.clientX, y: event.clientY };
  }

  // Offset to the middle of the element
  const x = rect.x + rect.width / 2;
  // Placed at the bottom of the element
  const y = rect.y + rect.height;
  return { x, y };
}

/**
 * @param {Event} event
 */
function deriveAction(event) {
  const target = /** @type {HTMLElement} */ (event.target);
  const hitALink = !!target.closest("a");
  if (target.closest("dfn:not([data-cite]), .index-term")) {
    return hitALink ? "none" : "show";
  }
  if (target.closest(".dfn-panel")) {
    if (hitALink) {
      return target.classList.contains("self-link") ? "hide" : "dock";
    }
    const panel = target.closest(".dfn-panel");
    return panel.classList.contains("docked") ? "hide" : "none";
  }
  if (document.querySelector(".dfn-panel:not([hidden])")) {
    return "hide";
  }
  return "none";
}

/**
 * @param {HTMLElement} dfn
 * @param {HTMLElement} panel
 * @param {{ x: number, y: number }} clickPosition
 */
function displayPanel(dfn, panel, { x, y }) {
  panel.hidden = false;
  // distance (px) between edge of panel and the pointing triangle (caret)
  const MARGIN = 20;

  const dfnRects = dfn.getClientRects();
  // Find the `top` offset when the `dfn` can be spread across multiple lines
  let closestTop = 0;
  let minDiff = Infinity;
  for (const rect of dfnRects) {
    const { top, bottom } = rect;
    const diffFromClickY = Math.abs((top + bottom) / 2 - y);
    if (diffFromClickY < minDiff) {
      minDiff = diffFromClickY;
      closestTop = top;
    }
  }

  const top = window.scrollY + closestTop + dfnRects[0].height;
  const left = x - MARGIN;
  panel.style.left = `${left}px`;
  panel.style.top = `${top}px`;

  // Find if the panel is flowing out of the window
  const panelRect = panel.getBoundingClientRect();
  const SCREEN_WIDTH = Math.min(window.innerWidth, window.screen.width);
  if (panelRect.right > SCREEN_WIDTH) {
    const newLeft = Math.max(MARGIN, x + MARGIN - panelRect.width);
    const newCaretOffset = left - newLeft;
    panel.style.left = `${newLeft}px`;
    /** @type {HTMLElement} */
    const caret = panel.querySelector(".caret");
    caret.style.left = `${newCaretOffset}px`;
  }

  // As it's a dialog, we trap focus.
  // TODO: when <dialog> becomes a implemented, we should really
  // use that.
  trapFocus(panel, dfn);
}

/**
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function trapFocus(panel, dfn) {
  /** @type NodeListOf<HTMLAnchorElement> elements */
  const anchors = panel.querySelectorAll("a[href]");
  // No need to trap focus
  if (!anchors.length) return;

  // Move focus to first anchor element
  const first = anchors.item(0);
  first.focus();

  const trapListener = createTrapListener(anchors, panel, dfn);
  panel.addEventListener("keydown", trapListener);

  // Hiding the panel releases the trap
  const mo = new MutationObserver(records => {
    const [record] = records;
    const target = /** @type HTMLElement */ (record.target);
    if (target.hidden) {
      panel.removeEventListener("keydown", trapListener);
      mo.disconnect();
    }
  });
  mo.observe(panel, { attributes: true, attributeFilter: ["hidden"] });
}

/**
 *
 * @param {NodeListOf<HTMLAnchorElement>} anchors
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function createTrapListener(anchors, panel, dfn) {
  const lastIndex = anchors.length - 1;
  let currentIndex = 0;
  return event => {
    switch (event.key) {
      // Hitting "Tab" traps us in a nice loop around elements.
      case "Tab": {
        event.preventDefault();
        currentIndex += event.shiftKey ? -1 : +1;
        if (currentIndex < 0) {
          currentIndex = lastIndex;
        } else if (currentIndex > lastIndex) {
          currentIndex = 0;
        }
        anchors.item(currentIndex).focus();
        break;
      }

      // Hitting "Enter" on an anchor releases the trap.
      case "Enter":
        hidePanel(panel);
        break;

      // Hitting "Escape" returns focus to dfn.
      case "Escape":
        hidePanel(panel);
        dfn.focus();
        return;
    }
  };
}

/** @param {HTMLElement} panel */
function hidePanel(panel) {
  if (!panel) return;
  panel.hidden = true;
  panel.classList.remove("docked");
}
})()</script><script src="https://www.w3.org/scripts/TR/2016/fixup.js"></script></body></html>